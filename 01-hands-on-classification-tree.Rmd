---
title: 'Decision Trees in R'
author: "JR Ferrer-Paris"
date: "09/07/2021"
editor_options:
  chunk_output_type: console
---
## Overview

Decision trees are recursive partitioning methods that divide the predictor spaces into simpler regions and can be visualized in a tree-like structure. They attempt to classify data by dividing it into subsets according to a Y output variable and based on some predictors.

## What data do we need?

- Y: The output or response variable is a categorical variable with two or more classes (in R: factor with two or more levels)
- X: A set of predictors or features, might be a mix of continuous and categorical variables, they should not have any missing values

### Load data

Here we will work with two examples.

First, we will use the _iris_ dataset from base R.
```{r}
data(iris)
str(iris)
```


We can load the Breast Cancer dataset from the _mlbench_ package
```{r}
require(mlbench)
data(BreastCancer)
str(BreastCancer)
```

## What package to use

Classification trees are implemented in packages:
- _tree_: Classification and Regression Trees
- _rpart_: Recursive Partitioning and Regression Trees

### Load packages

Here we will work with package _rpart_, and we will also load two additional packages for creating the plots

```{r load_packages}
library(rpart)
# auxiliary packages for plotting:
library(ggplot2)
library(rpart.plot)
```


## Fit a model

### _iris_ dataset
Let's start with a familiar dataset:

```{r}
set.seed(3)

tree = rpart::rpart(Species ~ ., data = iris,
             method = "class")
print(tree)
```

This is a very simple tree and we can walk through the output recognising three levels of nodes: (1) is the root node, (2) and (3) are the branches based on Petal Length. Branch (2) has 50 samples all belong to the first class (setosa), branch (3) has 100 samples of two different classes. Branch (3) splits into two further branches (6) and (7) based on petal width, these end-nodes (or leaf-nodes) have 54 and 46 samples respectively.

We can visualise the same information in a fancy _rpart.plot_:

```{r plot_tree}
rpart.plot::rpart.plot(tree)
```

When an end node only contains samples from a single class it is considered to be "pure". So the node for _I. setosa_ is pure, the others have 2 and 9% impurity.

### _Breast cancer_ dataset

Now let's look at a more challenging dataset.


```{r}
set.seed(3)

BC.data <- BreastCancer[,-1]

tree = rpart::rpart(Class ~ ., data = BC.data,
             method = "class")
print(tree)
```

This is a more complex tree with up to five levels of branching, can you see them?

The plot is a great visual aid, but what do all these values mean?

```{r plot_tree}
rpart.plot::rpart.plot(tree)
```

Here the output or response variable has two classes, so the rules are slightly simplified. Each box is labelled with the predominant class on top, the proportion of observations in the second class within each group (in this case 'malignant'), and the percentage of total observation within the group. Compare figure and text to try make sense of this.


We can also look inside of `tree` to see what we can unpack. "variable.importance" is one we should check out!

```{r}
names(tree)
tree$variable.importance
```

Plot variable importance

```{r}
# Turn the tree$variable.importance vector into a dataframe
tree_varimp = data.frame(tree$variable.importance)

# Add rownames as their own column
tree_varimp$x = rownames(tree_varimp)

# Reorder clumns
tree_varimp = tree_varimp[, c(2,1)]

# Reset row names
rownames(tree_varimp) = NULL

# Rename columns
names(tree_varimp) = c("Variable", "Importance")
tree_varimp

# Plot
ggplot(tree_varimp, aes(x = reorder(Variable, Importance),
                        y = Importance)) +
  geom_bar(stat = "identity") +
  theme_bw() + coord_flip() + xlab("")
```

In decision trees the main hyperparameter (configuration setting) is the **complexity parameter** (CP), but the name is a little counterintuitive; a high CP results in a simple decision tree with few splits, whereas a low CP results in a larger decision tree with many splits.

`rpart` uses cross-validation internally to estimate the accuracy at various CP settings. We can review those to see what setting seems best.

Print the results for various CP settings - we want the one with the lowest "xerror". We can also plot the performance estimates for different CP settings.

```{r plotcp_tree}
# Show estimated error rate at different complexity parameter settings.
printcp(tree)

# Plot those estimated error rates.
plotcp(tree)

# Trees of similar sizes might appear to be tied for lowest "xerror", but a tree with fewer splits might be easier to interpret.

tree_pruned2 = prune(tree, cp = 0.028986) # 2 splits

tree_pruned6 = prune(tree, cp = 0.010870) # 6 splits
```

Print detailed results, variable importance, and summary of splits.

```{r}
summary(tree_pruned2)
rpart.plot(tree_pruned2)
```
```{r}
summary(tree_pruned6)
rpart.plot(tree_pruned6)
```
