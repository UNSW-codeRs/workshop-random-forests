---
title: "Random Forest in R"
author: "JR Ferrer-Paris"
date: "28/07/2021"
editor_options:
  chunk_output_type: console
---

## Overview

The random forest algorithm seeks to improve on the performance of a single decision tree by taking the average of many trees. Thus, a random forest can be viewed as an **ensemble** method, or model averaging approach. The algorithm was invented by UC Berkeley's own Leo Breiman in 2001, who was also a co-creator of decision trees (see his [1984 CART book](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)).

Random forests are an extension of **bagging**, in which multiple samples of the original data are drawn with replacement (aka "bootstrap samples"). An algorithm is fit separately to each sample, then the average of those estimates is used for prediction. While bagging can be used by any algorithm, random forest uses decision trees as its base learner. Random forests add another level of randomness by also randomly sampling the features (or covariates) at each split in each decision tree. This makes the decision trees use different covariates and therefore be more unique. As a result, the average of these trees tends to be more accurate overall.

## What data do we need?

The same as any regular classification decision tree!

- Y: The output or response variable is a categorical variable with two or more classes (in R: factor with two or more levels)
- X: A set of predictors or features, might be a mix of continuous and categorical variables, they should not have any missing values

### Load data

Here we will work again with two examples.

First, we will use the _iris_ dataset from base R.
```{r}
data(iris)
str(iris)
```


We can load the Breast Cancer dataset from the _mlbench_ package
```{r}
require(mlbench)
data(BreastCancer)
str(BreastCancer)
```



## What package to use

Random Forests are implemented in several packages:
- _randomForest_: Breiman and Cutler's Random Forests for Classification and Regression
- _ranger_: A Fast Implementation of Random Forests
- _party_: A Laboratory for Recursive Partytioning
- _RandomForestsGLS_: Random Forests for Dependent Data
- _randomForestSRC_: Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC)

Some helper packages are:
- _randomForestExplainer_: Explaining and Visualizing Random Forests in Terms of Variable Importance
- _vip_: Variable Importance Plots
- _varImp_: Variable Importance Plots
- _tidymodels_: tidy model framework

### Load packages

Here we will work with package _randomForest_, and we will also load one additional packages for creating the plots

```{r load_packages}
library(randomForest)
# auxiliary packages for plotting:
library(vip)
```


## Fit a model

### _iris_ dataset
Let's start with a familiar dataset:

```{r}
set.seed(3)

rf_model = randomForest(Species ~ ., data = iris, ntree=30)
print(rf_model)
```

The "OOB estimate of error rate" shows us how accurate our model is. $accuracy = 1 - error rate$. OOB stands for "out of bag" - and bag is short for "bootstrap aggregation". So OOB estimates performance by comparing the predicted outcome value to the actual value across all trees using only the observations that were not part of the training data for that tree.

So a forest is made up of trees, right?

Let's see tree number three:
```{r}
getTree(rf_model, 3, labelVar=TRUE)
```

Why are these trees so complex? We can control the complexity of the trees with `nodesize` and `maxnode` parameters. For example a large node size and small number of nodes will result in simpler/shorter trees, but this could increase OOB error rates:

```{r}
rf2 = randomForest(Species ~ ., data = iris, ntree=30, nodesize=20, maxnodes=5)
print(rf2)
getTree(rf2, 3, labelVar=TRUE)

```

We can also control the number of variables sampled in each split with `mtry`, by default randomForest will test one third of the variables in each split and choose the best one. Here we can use all four variables each time:


```{r}
rf3 = randomForest(Species ~ ., data = iris, ntree=30, mtry=4)
print(rf3)
```

These three hyper-parameters can be tuned to get better results:

```{r}
for (ns in c(5,25)) {
  for (mn in c(5,15)) {
  for (mt in c(2,4)) {
  rf1 <- randomForest(Species ~ ., data = iris, nodesize=ns, maxnodes=mn, mtry=mt, ntree=30)  
  cat(sprintf("nodesize=%02d maxnodes=%02d mtry=%s OOB-error=%0.4f\n", ns, mn,mt, rf1$err.rate[30,"OOB"]))
  }
  }
}
```

However this is subject to variability due to randomness, and a better fine-tuning requires several replicate runs of each combinations. Some functions in packages ... can be used to do this more efficiently.


### _Breast cancer_ dataset

Now let's look at a more challenging dataset.


```{r}
set.seed(3)

BC.data <- subset(BreastCancer[,-1],!is.na(Bare.nuclei))

rf_model = randomForest(Class ~ ., data = BC.data,importance=TRUE)
print(rf_model)
```




We can examine the relative variable importance in table and graph form, but without all the hard coding that we did in 04-decision-trees.Rmd. Random Forest estimates variable importance by separately examining each variable and estimating how much the model's accuracy drops when that variable's values are randomly shuffled (permuted). The shuffling temporarily removes any relationship between that covariate's value and the outcome. If a variable is important then the model's accuracy will suffer a large drop when it is randomly shuffled. But if the model's accuracy doesn't change it means the variable is not important to the model - e.g. maybe it was never even chosen as a split in any of the decision trees.


```{r plot_tree}
importance(rf_model)
```

The first measure is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. If the standard deviation of the differences is equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that case).

The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares.


Read up on the [gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient) here. It's basically a measure of diversity or dispersion - a higher gini means the model is classifying better. The gini version does not randomly shuffle the variable values.  

Notice for example that Bl.cromatin and Normal.nucleoli have similar values of mean decrease in overall accuracy when permuted, but they have different importance for each class.  
```{r plot_tree}
importance(rf_model)
```

```{r}
varImpPlot(rf_model,type=1)
```

```{r}
vip(rf_model,type=1)
```


Now, the goal is to see how the model performs on the test dataset:



We can also look inside of `tree` to see what we can unpack. "variable.importance" is one we should check out!

```{r}
names(rf_model)
str(rf_model$forest)
```

Plot variable importance

```{r}
str(rf_model$forest)
```


